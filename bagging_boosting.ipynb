{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Aggregated Bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a common approach used mostly in tree models but can be used in classic regression and classification settings. Bagging is based on the basic principle of statistics: Let's assume we have n indepenedent observations $O_1, O_2, ..., O_n$, each with a variance of $\\sigma^2$, the variance of mean $\\overline{O}$ of n observations is $\\frac{\\sigma^2}{n}$. Thus, averaging reduces the overall variance. Decision Trees have high variance in terms of prediction i.e, if we split a dataset into two halves and fit our tree separately on both data sets, the results would vary a lot. In contrast, classic regression have low variance. So, many times, approaches such as bagging is used in order to reduce the variance of Decision Trees and get stable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same aforementioned concept, we can train n different decision tree models on n different training datasets and in order to make predictions, we can take the average of all trained trees (in case of regression) or majority voting of n different trees (in case of classification). But in real scenario, we do not have n different training data sets. We have only one training data set. Thus, insted of using n different training data sets, we use bootstraped samples (taking out samples with replacements) of our training dataset. We build our model on each of the bootstraped samples and then average the results over all models in case of regression or majority voting in case of classification. This overall process is known as Bagging. In general, these trees are deep (no pruning), resulting in high variance and low bias but averaging them together reduces the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Estimation (Out-of-Bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error estimation for bagged models can be done easily without performing cross validation or validation set. As bagging is done by fitting a model to bootstrapped samples, on an average each model uses only two-thirds of the observations, rest one-thirds are not used and are termed as out-of-bag (OOB) observations. Thus, we can make prediction for each observation using models for which it was OOB and average it across all models (for regression) or take majority (for classification). In this way we get the prediciton for each of the obervation. Once we have prediction for each observation, we can calculate the MSE (regreesion) or classification error (classification) and estimate the test accuracy of our bagged model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike bagging where each tree is build independent of another on a subset of observations, in boosting trees are built in a sequential manner. Each tree uses the information from the previous tree. In bagging we used bootstrapped samples of observations but in case of boosting each tree is fit on a modified version of the original data. Boosting is done to decrease the bias. In case of regression, each model is fitted to the residuals from the previous one and thus improving upon the performance of the previous model. In case of classification, the observations are given weights and misclassified observations from the previous model are weighted more than correctly classified observations, Thus, the next model tries to classify misclassified observations more accurately. Boosting algorithms come in varying forms. AdaBoost, Gradient Boosting, XgBoost are among most commonly used boosting algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
